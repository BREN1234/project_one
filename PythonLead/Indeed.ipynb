{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd,re,requests,time\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from notifier import sendErrorMsg\n",
    "\n",
    "#returns the daily_id from dailytable\n",
    "def getDailyIdFromDailyTable(engine,record_df):\n",
    "    #get Cursor\n",
    "    record_df.to_sql('daliytable', engine, if_exists='append',index=False)\n",
    "    #alter the sequence of the id i.e. restarting from 1\n",
    "    #ALTER SEQUENCE <tablename>_<id>_seq RESTART WITH 1\n",
    "    with engine.connect() as connection:\n",
    "        result_set=connection.execute(\"SELECT id from daliytable order by id desc limit 1;\")\n",
    "        for i in result_set:\n",
    "            return(i['id'])\n",
    "\n",
    "#this function gets portal id from websiteportal \n",
    "def getPortalIdFromWebsitePortal(engine):\n",
    "    with engine.connect() as connection:\n",
    "        result_set= connection.execute(\"SELECT id from websiteportal where portal = 'Indeed' limit 1;\")\n",
    "        for i in result_set:\n",
    "            return(i['id'])\n",
    "\n",
    "#inseting into recprds table\n",
    "def insertRecordsToRecords(engine,data_matrix):\n",
    "    data_matrix.to_sql('records',engine,if_exists='append',index=False)\n",
    "    print(\"inserting success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':    \n",
    "    dat=datetime.datetime.today()\n",
    "    start=0\n",
    "    lengthOfRecord=0\n",
    "    numberOfJobs=0\n",
    "    dataFrame=pd.DataFrame()\n",
    "    df=pd.DataFrame()\n",
    "    date=[]\n",
    "    company=[]\n",
    "    location=[]\n",
    "    summary=[]\n",
    "    posted_date=[]\n",
    "    posted_day=[]\n",
    "    posted_links=[]\n",
    "    companyUrl=[]\n",
    "    designation=[]\n",
    "    countries=['Canada','Singapore','USA','UK','Ireland','Bahrain','Oman','Qatar']\n",
    "    for job in ['jaspersoft','Talend']:\n",
    "        appendurl='/jobs?as_and='+str(job)+'&as_phr=\"Talend\"&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&salary=&radius=25&l=&fromage=any&limit=50&start=%s&sort=date&psf=advsrch'\n",
    "        urls=['https://ca.indeed.com','https://www.indeed.com.sg','https://www.indeed.com','https://www.indeed.co.uk',\\\n",
    "              'https://ie.indeed.com','https://bh.indeed.com','https://om.indeed.com','https://qa.indeed.com']\n",
    "        try:\n",
    "            urls=[url1,url2,url3,url4,url5,url6,url7,url8]\n",
    "            for url,country in zip(urls,countries):\n",
    "                counter=0\n",
    "                while lengthOfRecord <= numberOfJobs:\n",
    "                    urll= ((url+appendurl)%start)\n",
    "                    soup= BeautifulSoup(requests.get(urll).text,'lxml')\n",
    "                    numberOfHits= soup.find(\"div\",id=\"searchCount\").text\n",
    "                    numberOfHits= [re.sub(r'[^A-Za-z0-9]+', '', x) for x in numberOfHits.strip().split(\" \")]\n",
    "                    if len(numberOfHits) == 6:\n",
    "                        numberOfJobs=int(numberOfHits[5])\n",
    "                    if len(numberOfHits) == 5:\n",
    "                        numberOfJobs=int(numberOfHits[3])\n",
    "                    print(\"Number of Jobs: \",numberOfJobs,\"record count: \",start,\" Job: \",job,\"country: \",country)\n",
    "                    result= soup.find_all(\"div\",class_=\"row result clickcard\".split())\n",
    "                    lengthOfRecord= lengthOfRecord+len(result)\n",
    "                    if lengthOfRecord <= numberOfJobs:\n",
    "                        total=numberOfJobs\n",
    "                        for firstLink in result:\n",
    "                            try:\n",
    "                                links= firstLink.find('a',{\"class\":\"turnstileLink\"})\n",
    "                                date.append(dat.strftime('%d-%m-%Y'))\n",
    "                                company.append(firstLink.find('span',{\"class\":\"company\"}).text.strip())\n",
    "                                \n",
    "                                location.append(firstLink.find('span',{\"class\":\"location\"}).text.strip())\n",
    "                                \n",
    "                                summary.append(firstLink.find('span',{\"class\":\"summary\"}).text.strip())\n",
    "                                \n",
    "                                companyl= firstLink.find('span',{\"class\":\"company\"}).findChildren()\n",
    "                                #print(firstLink.find('span',{\"class\":\"date\"}))\n",
    "                                try:\n",
    "                                    l=firstLink.find('span',{\"class\":\"date\"}).text\n",
    "                                except Exception as e:\n",
    "                                    print(e)\n",
    "                                posted_day.append(l)\n",
    "                                dt=None\n",
    "                                \n",
    "                                li=re.sub(r'[^0-9A-Z]','',l)\n",
    "                                \n",
    "                                #print(li,li.find('hour',0,len('hour')))\n",
    "                                try:\n",
    "                                    if li.isalpha():\n",
    "                                        dt=dat.date()\n",
    "                                    else:\n",
    "                                        dt=(dat-datetime.timedelta(days=int(li))).date()\n",
    "                                except ValueError as e:\n",
    "                                    print(\"i am out\")\n",
    "                                    print(e)\n",
    "                               \n",
    "                                posted_date.append(dt)\n",
    "                                posted_links.append(url+links['href'])\n",
    "                                designation.append(links['title'])\n",
    "                                \n",
    "                                if (len(companyl) != 0):\n",
    "                                    for i in companyl:\n",
    "                                        try:\n",
    "                                            companyUrl.append(url+i['href'])\n",
    "                                        except KeyError as e:\n",
    "                                            print(\"KeyError: Not a Problem\",e)\n",
    "                                else:\n",
    "                                    companyUrl.append(\"NULL\")\n",
    "                            except Exception as e:\n",
    "                                break;\n",
    "                                print(e)\n",
    "                                \n",
    "                    else:\n",
    "                        break\n",
    "                    counter +=1\n",
    "                    start=start+50\n",
    "                print(len(company),len(location),len(summary),len(posted_date),len(posted_day),len(posted_links),len(companyUrl))\n",
    "                df['posted_date']=posted_date\n",
    "                df['posted_day']=posted_day\n",
    "                df['posted_links']=posted_links\n",
    "                df['designation']=designation\n",
    "                df['company_name']=company\n",
    "                df['companyurl']=companyUrl\n",
    "                df['location']= [ loc+' '+ job for loc in location]\n",
    "                df['summary']=summary \n",
    "                print(\"DF shape and number of Jobs: \",df.shape,numberOfJobs)\n",
    "\n",
    "                dataFrame=dataFrame.append(df)   \n",
    "                start=0      #setting again 0 to new searches\n",
    "                lengthOfRecord=0\n",
    "                numberOfJobs=0\n",
    "                df=pd.DataFrame()\n",
    "                numberOfJobs=0\n",
    "                date=[]\n",
    "                company=[]\n",
    "                location=[]\n",
    "                summary=[]\n",
    "                posted_date=[]\n",
    "                posted_day=[]\n",
    "                posted_links=[]\n",
    "                companyUrl=[]\n",
    "                designation=[]\n",
    "        except Exception as e:\n",
    "            print(e,\"asdfasdf asdf\",url)\n",
    "            \n",
    "\n",
    "dataFrame.to_csv(\"C:/Datasets/JobData/daily_indeed_\"+str(dat.strftime('%d_%m_%Y'))+\".csv\")\n",
    "#framing the table and crawled date for DB\n",
    "record=[(dat.date(),'daily_indeed_'+str(dat.strftime('%d_%m_%Y')+\".csv\"))]\n",
    "record_df=pd.DataFrame(record,columns=['crawled_date','filename'])\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://postgres:postgres@localhost:5432/job_data')\n",
    "daily_id=getDailyIdFromDailyTable(engine,record_df)\n",
    "portal_id=getPortalIdFromWebsitePortal(engine)\n",
    "dataMatrix=dataFrame\n",
    "dataMatrix['day_id']=daily_id\n",
    "dataMatrix['portal_id']=portal_id\n",
    "dataMatrix['posted_date']=dataMatrix['posted_date'].apply(lambda d : datetime.datetime.strptime(d,\"%Y-%m-%d\").date() if isinstance(d, str) else d)\n",
    "insertRecordsToRecords(engine,dataMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
